{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests\n",
    "from PIL import Image, ImageSequence, ImageOps\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be scraping meleeframedata.com for the amount of damage coming from each attack. After examining the output of the \"damage\" wrappers on the website, we construct the following function to return a floating point number from the raw text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDamage(s : str) -> str:\n",
    "   firstNumberChars = [char for char in s.split(' ')[0] if char.isdigit() or char == '.']\n",
    "   if not firstNumberChars:\n",
    "      return 0\n",
    "   return float(''.join(firstNumberChars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell determines which types of attacks we will fetch. Each normal move can be modified to be an aerial or a special, and the div classes in the html file are modified by the outputs of the dictionary `typeToDivClass`. We can also add more attributes to the list `attributesToFetch` in order to scrape more or fewer details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_types = ['Normal', 'Aerial', 'Special'] \n",
    "typeToDivClass = {'Normal': '', 'Aerial':'air-', 'Special': 'special-'}\n",
    "attributesToFetch = ['percent', 'name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The homepage of the website contains links to each character's webpage. Upon inspection of the HTML source, we see that the first 15 links of the page are generic navigation links. We thus append the rest of the links to a list of urls of character pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homeurl = 'https://meleeframedata.com/'\n",
    "hr = requests.get(homeurl, verify=False) \n",
    "homePageSoup  = BeautifulSoup(hr.content, 'html5lib') # If this line causes an error, run 'pip install html5lib' or install html5lib \n",
    "links = homePageSoup.find_all('a')\n",
    "suffixes = []\n",
    "for i in links[16:]:\n",
    "    suffixes.append(str(i).split('\"')[1].replace('amp;', ''))\n",
    "urls = ['https://meleeframedata.com/' + suffix for suffix in suffixes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in each characters page, we can go through and scrape each attack listed on the website. We save the character name, move name and parsed damage amount in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfMoves = []\n",
    "for charPage in urls:\n",
    "    req = requests.get(charPage, verify = False)\n",
    "    charSoup = BeautifulSoup(req.content, 'html5lib')\n",
    "    charName = charSoup.find('title').text\n",
    "    for type in move_types:\n",
    "        modifier = typeToDivClass[type]\n",
    "        container = modifier + 'move-container'\n",
    "        percent = modifier + 'percent'\n",
    "        name = modifier + 'movename'\n",
    "        for move in charSoup.findAll(class_ = container):\n",
    "            listOfMoves.append([charName, move.findAll(class_= name)[0].text, parseDamage(move.findAll(class_= percent)[0].text)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save this scraped data as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(listOfMoves, columns=['Character', 'Move', 'Damage'])\n",
    "df.to_pickle('moveDamages.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moves which deal 0 damage tend to have very unique effects which are difficult to predict; we will preemptively sanitize our training set by removing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['Damage'] != 0]\n",
    "df.sort_values('Damage').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images corresponding to attack animations are stored in an index page at the URL below. We will pass through this index and append a link to each folder in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://meleeframedata.com/static/gifs/'\n",
    "reqs = requests.get(url, verify=False)\n",
    "soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "urls = []\n",
    "for link in soup.find_all('a')[5:]:\n",
    "    urls.append(url + link.get('href'))\n",
    "urls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how the actual image files in the index are formatted so that we can pick them apart when saving them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowserURL = urls[0]\n",
    "bowserReq = requests.get(bowserURL, verify=False)\n",
    "bowSoup = BeautifulSoup(bowserReq.text, 'html.parser')\n",
    "bowserAttackurls = []\n",
    "for bowLink in bowSoup.find_all('a')[5:]:\n",
    "    bowserAttackurls.append(bowserURL + bowLink.get('href'))\n",
    "bowserAttackurls[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also format each folder in local storage from the url from which it is obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/gfi8p/imageClassifier/rawimages/bowser/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'C:/Users/gfi8p/imageClassifier/'\n",
    "A = PATH + 'rawimages/' + urls[0].split('/')[-2] + '/'\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block of code, the outer for loop will create a folder for each character. The inner for loops will obtain URLS for images relevant to that character, then save each image into the created folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for charURL in urls:\n",
    "    folder = PATH + 'rawimages/' + charURL.split('/')[-2] + '/'\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "    charRequest = requests.get(charURL)\n",
    "    charSoup = BeautifulSoup(charRequest.text, 'html.parser')\n",
    "    charAttackURLs = []\n",
    "    for charAttack in charSoup.find_all('a')[5:]:\n",
    "        charAttackURLs.append(charURL + charAttack.get('href'))\n",
    "    for url in charAttackURLs:\n",
    "        response = requests.get(url)\n",
    "        filename = folder + url.split('/')[-1]\n",
    "        if response.status_code == 200:\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all of the images on the local machine and it is time to move on to processing the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
